{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ys2UhvuovahV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j8-Q2KCvmWL"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdtstHYiwDbc",
    "outputId": "d0f81dff-10a0-45e2-ad83-67aa936e3573"
   },
   "outputs": [],
   "source": [
    "sample_count = 150\n",
    "\n",
    "X_train = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "\n",
    "# Leaving only labels 0 and 1\n",
    "idx = np.append(\n",
    "    np.where(X_train.targets == 0)[0][:sample_count],\n",
    "    np.where(X_train.targets == 1)[0][:sample_count],\n",
    ")\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True)\n",
    "\n",
    "# Test set\n",
    "sample_count = 200\n",
    "\n",
    "X_test = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "idx = np.append(\n",
    "    np.where(X_test.targets == 0)[0][:sample_count],\n",
    "    np.where(X_test.targets == 1)[0][:sample_count],\n",
    ")\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]\n",
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTuVc1L0sGUF"
   },
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    \"\"\"This class defines the quantum circuit structure and the run method which is used to calculate an expectation value\"\"\"\n",
    "\n",
    "    def __init__(self, qubit_count: int):\n",
    "        \"\"\"Define the quantum circuit in CUDA Quantum\"\"\"\n",
    "\n",
    "        kernel, thetas = cudaq.make_kernel(list)\n",
    "\n",
    "        self.kernel = kernel\n",
    "\n",
    "        self.theta = thetas\n",
    "\n",
    "        qubits = kernel.qalloc(qubit_count)\n",
    "\n",
    "        self.kernel.h(qubits)\n",
    "\n",
    "        # Variational gate parameters which are optimised during training\n",
    "        for i in range(qubit_count):\n",
    "            kernel.rx(thetas[i], qubits[i])\n",
    "\n",
    "\n",
    "        hamiltonian = spin.z(0)\n",
    "        for i in range(1, qubit_count):\n",
    "            hamiltonian *= spin.z(i)\n",
    "        # print(hamiltonian)\n",
    "        self.hamiltonian = hamiltonian\n",
    "\n",
    "    def run(self, thetas: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Excetute the quantum circuit to output an expectation value\"\"\"\n",
    "        expectation = torch.tensor(cudaq.observe(self.kernel, self.hamiltonian,\n",
    "                                                 thetas).expectation(),\n",
    "                                   device=device)\n",
    "\n",
    "        return expectation\n",
    "class QuantumFunction(Function):\n",
    "    \"\"\"Allows the quantum circuit to pass data through it and compute the gradients\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, thetas: torch.tensor, quantum_circuit,\n",
    "                shift) -> torch.tensor:\n",
    "        # Save shift and quantum_circuit in context to use in backward\n",
    "        ctx.shift = shift\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "\n",
    "        # Calculate exp_val\n",
    "        expectation_z = ctx.quantum_circuit.run(thetas)\n",
    "\n",
    "        ctx.save_for_backward(thetas, expectation_z)\n",
    "        #print(expectation_z)\n",
    "        return expectation_z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Backward pass computation via finite difference parameter shift\"\"\"\n",
    "\n",
    "        thetas, expectation_z = ctx.saved_tensors\n",
    "\n",
    "        gradients = torch.zeros(len(thetas), device=device)\n",
    "\n",
    "        for i in range(len(thetas)):\n",
    "            shift_right = torch.clone(thetas)\n",
    "\n",
    "            shift_right[i] += ctx.shift\n",
    "\n",
    "            shift_left = torch.clone(thetas)\n",
    "\n",
    "            shift_left[i] -= ctx.shift\n",
    "\n",
    "            expectation_right = ctx.quantum_circuit.run(shift_right)\n",
    "            expectation_left = ctx.quantum_circuit.run(shift_left)\n",
    "\n",
    "            gradients[i] = 0.5 * (expectation_right - expectation_left)\n",
    "\n",
    "        return gradients * grad_output.float(), None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDPGt5KgwOsj"
   },
   "outputs": [],
   "source": [
    "class QuantumLayer(nn.Module):\n",
    "    \"\"\"Encapsulates a quantum circuit and a quantum function into a quantum layer\"\"\"\n",
    "\n",
    "    def __init__(self, shift: torch.tensor):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.quantum_circuit = QuantumCircuit(16)\n",
    "        self.shift = shift\n",
    "\n",
    "    def forward(self, input):\n",
    "        ans = QuantumFunction.apply(input, self.quantum_circuit, self.shift)\n",
    "        #print(ans)\n",
    "        return ans\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Neural network structure\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(256, 16)\n",
    "        #self.fc2 = nn.Linear(64, 12)  # Output a 2D tensor since we have 2 variational parameters in our quantum circuit\n",
    "        self.hybrid = QuantumLayer(\n",
    "            torch.tensor(np.pi / 2)\n",
    "        )  # Input is the magnitude of the parameter shifts to calculate gradients\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(1, -1)\n",
    "        x = F.relu(self.fc1(x)).reshape(-1)\n",
    "        #x = self.fc2(x).reshape(-1)  # Reshapes required to satisfy input dimensions to CUDAQ\n",
    "        x = self.hybrid(x).reshape(-1)\n",
    "        #print(x)\n",
    "        return torch.cat((x, 1 - x), -1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YhrDaiiwYON",
    "outputId": "aecbc54d-d593-4327-aca4-1bdc995bd2fc"
   },
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_func = nn.NLLLoss().to(device)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "epoch_loss = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # batch training\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data).to(device)\n",
    "\n",
    "        # Calculating loss\n",
    "        loss = loss_func(output, target).to(device)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    epoch_loss.append(batch_loss / batch_idx)\n",
    "\n",
    "    print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(\n",
    "        100.0 * (epoch + 1) / epochs, epoch_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "3lcriFWuwacw",
    "outputId": "7229c3c9-004d-403b-821b-f4258a291cf7"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)\n",
    "plt.title(\"Hybrid NN Training Convergence\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "\n",
    "plt.ylabel(\"Neg Log Likelihood Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjnpsLC_wezJ",
    "outputId": "a060a503-9959-42fe-fbbd-5dffe3a1b589"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data).to(device)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\"Performance on test data:\\n\\tAccuracy: {:.1f}%\".format(\n",
    "        correct / len(test_loader) * 100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
