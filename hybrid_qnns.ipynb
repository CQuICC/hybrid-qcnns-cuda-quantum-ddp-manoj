{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Quantum Neural Networks \n",
    "\n",
    "The example below highlights a hybrid quantum neural network workflow with CUDA Quantum and Pytorch where both layers are GPU accelerated to maximise performance. \n",
    "\n",
    "\n",
    "<img src=\"images/hybrid.png\" alt=\"hybrid\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform binary classification on the MNIST dataset where data flows through the neural network architecture to the quantum circuit whose output is used to classify hand written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: torch in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (0.16.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: filelock in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\n",
      "Requirement already satisfied: requests in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/manoj_raga/miniforge3/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Invalid simulator requested: custatevec_fp32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcudaq\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcudaq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spin\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/cudaq/__init__.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find a suitable cuQuantum Python package.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pycudaq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdomains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chemistry\n\u001b[1;32m     31\u001b[0m initKwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mImportError\u001b[0m: Invalid simulator requested: custatevec_fp32"
     ]
    }
   ],
   "source": [
    "# Import the relevant packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cudaq\n",
    "from cudaq import spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU utilities\n",
    "\n",
    "cudaq.set_target(\"nvidia\")  # Set CUDAQ to run on GPU's\n",
    "\n",
    "torch.cuda.is_available(\n",
    ")  # If this is True then the NVIDIA drivers are correctly installed\n",
    "\n",
    "torch.cuda.device_count()  # Counts the number of GPU's available\n",
    "\n",
    "torch.cuda.current_device()\n",
    "\n",
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "sample_count = 140\n",
    "\n",
    "X_train = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "\n",
    "# Leaving only labels 0 and 1\n",
    "idx = np.append(\n",
    "    np.where(X_train.targets == 0)[0][:sample_count],\n",
    "    np.where(X_train.targets == 1)[0][:sample_count],\n",
    ")\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(X_train, batch_size=1, shuffle=True)\n",
    "\n",
    "# Test set\n",
    "sample_count = 70\n",
    "\n",
    "X_test = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")\n",
    "idx = np.append(\n",
    "    np.where(X_test.targets == 0)[0][:sample_count],\n",
    "    np.where(X_test.targets == 1)[0][:sample_count],\n",
    ")\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(X_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumCircuit:\n",
    "    \"\"\"This class defines the quantum circuit structure and the run method which is used to calculate an expectation value\"\"\"\n",
    "\n",
    "    def __init__(self, qubit_count: int):\n",
    "        \"\"\"Define the quantum circuit in CUDA Quantum\"\"\"\n",
    "\n",
    "        kernel, thetas = cudaq.make_kernel(list)\n",
    "\n",
    "        self.kernel = kernel\n",
    "\n",
    "        self.theta = thetas\n",
    "\n",
    "        qubits = kernel.qalloc(qubit_count)\n",
    "\n",
    "        self.kernel.h(qubits)\n",
    "\n",
    "        # Variational gate parameters which are optimised during training\n",
    "        kernel.ry(thetas[0], qubits[0])\n",
    "        kernel.rx(thetas[1], qubits[0])\n",
    "\n",
    "    def run(self, thetas: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Excetute the quantum circuit to output an expectation value\"\"\"\n",
    "\n",
    "        expectation = torch.tensor(cudaq.observe(self.kernel, spin.z(0),\n",
    "                                                 thetas).expectation(),\n",
    "                                   device=device)\n",
    "\n",
    "        return expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumFunction(Function):\n",
    "    \"\"\"Allows the quantum circuit to pass data through it and compute the gradients\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, thetas: torch.tensor, quantum_circuit,\n",
    "                shift) -> torch.tensor:\n",
    "        # Save shift and quantum_circuit in context to use in backward\n",
    "        ctx.shift = shift\n",
    "        ctx.quantum_circuit = quantum_circuit\n",
    "\n",
    "        # Calculate exp_val\n",
    "        expectation_z = ctx.quantum_circuit.run(thetas)\n",
    "\n",
    "        ctx.save_for_backward(thetas, expectation_z)\n",
    "\n",
    "        return expectation_z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Backward pass computation via finite difference parameter shift\"\"\"\n",
    "\n",
    "        thetas, expectation_z = ctx.saved_tensors\n",
    "\n",
    "        gradients = torch.zeros(len(thetas), device=device)\n",
    "\n",
    "        for i in range(len(thetas)):\n",
    "            shift_right = torch.clone(thetas)\n",
    "\n",
    "            shift_right[i] += ctx.shift\n",
    "\n",
    "            shift_left = torch.clone(thetas)\n",
    "\n",
    "            shift_left[i] -= ctx.shift\n",
    "\n",
    "            expectation_right = ctx.quantum_circuit.run(shift_right)\n",
    "            expectation_left = ctx.quantum_circuit.run(shift_left)\n",
    "\n",
    "            gradients[i] = 0.5 * (expectation_right - expectation_left)\n",
    "\n",
    "        return gradients * grad_output.float(), None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(nn.Module):\n",
    "    \"\"\"Encapsulates a quantum circuit and a quantum function into a quantum layer\"\"\"\n",
    "\n",
    "    def __init__(self, shift: torch.tensor):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.quantum_circuit = QuantumCircuit(1)  # 1 qubit quantum circuit\n",
    "        self.shift = shift\n",
    "\n",
    "    def forward(self, input):\n",
    "        ans = QuantumFunction.apply(input, self.quantum_circuit, self.shift)\n",
    "\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Neural network structure\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(256, 64)\n",
    "        self.fc2 = nn.Linear(\n",
    "            64, 2\n",
    "        )  # Output a 2D tensor since we have 2 variational parameters in our quantum circuit\n",
    "        self.hybrid = QuantumLayer(\n",
    "            torch.tensor(np.pi / 2)\n",
    "        )  # Input is the magnitude of the parameter shifts to calculate gradients\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(1, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x).reshape(\n",
    "            -1)  # Reshapes required to satisfy input dimensions to CUDAQ\n",
    "        x = self.hybrid(x).reshape(-1)\n",
    "\n",
    "        return torch.cat((x, 1 - x), -1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We move our model to the CUDA device to minimise data transfer between GPU and CPU\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_func = nn.NLLLoss().to(device)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "epoch_loss = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    batch_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):  # batch training\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data).to(device)\n",
    "\n",
    "        # Calculating loss\n",
    "        loss = loss_func(output, target).to(device)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimize the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss += loss.item()\n",
    "\n",
    "    epoch_loss.append(batch_loss / batch_idx)\n",
    "\n",
    "    print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(\n",
    "        100.0 * (epoch + 1) / epochs, epoch_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)\n",
    "plt.title(\"Hybrid NN Training Convergence\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "\n",
    "plt.ylabel(\"Neg Log Likelihood Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing on the test set\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data).to(device)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    print(\"Performance on test data:\\n\\tAccuracy: {:.1f}%\".format(\n",
    "        correct / len(test_loader) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
